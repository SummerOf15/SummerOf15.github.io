---
layout: _hight
title: K-L Divergence
date: 2018-10-24 16:12:26
mathjax: true
tags: denseCRF
excerpt_separator: <!--more-->
---
#### Definition
在介绍K-L散度之前，首先了解一下信息论以及相关的一些定义。

信息论中的基本想法是一个发生概率很低的事件发生了比起一件经常发生的事件发生带来的信息量更多。用一个例子来说明：“今天太阳从东边升起”中的信息量没有“今天发生了日食”的信息量大。从“今天发生日食”这句话中我们能够推断出来更多的信息，比如：日月地球在同一条线上。
<!--more-->
用一种数学的方式来表达这种信息量就是：
`自信息`:如对事件X=x的自信息:$$I(x)=-log(P(x))$$自信息通常是指对于一个事件的输出，也就是一个事件所蕴含的信息。
`香农熵`:针对一个概率分布中的不确定总量(信息量)进行量化。$$H(x)=-\sum_xP(x)log(P(x))$$换句话说就是：一个分布的香农熵是指遵循这个分布的事件所产生的期望信息总量。
`KL散度`:如果对于随机变量X有两个单独的概率分布P(x)和Q(x)，KL散度可以用来衡量这两个分布的差异，如P分布对Q分布的KL散度：$$D_{KL}(P||Q)=\sum_xP(x)[log(P(x))-log((Q(x))]$$
从信息传递方面解释KL散度是指：当我们使用一种被设计成能够使概率分布Q产生的消息的长度最小的编码时，用这个编码来传递包含概率分布P产生的符号的信息时，所需要的额外的信息量。
#### Properties
##### 非负性
KL散度是非负的。当$D_{KL}(P||Q)=0$时，代表P和Q的分布完全相同。
##### 非对称性
KL散度和距离不同之处在于KL散度不是对称的，如$D_{KL}(P||Q)$和$D_{KL}(Q||P)$不一样。从我的理解上说一个是用Q来逼近P，一个是用P来逼近Q，在当两个分布不一样的时候，逼近时采用的策略也不一样。详情可以参看下图。
[![i26jts.png](https://s1.ax1x.com/2018/10/30/i26jts.png)](https://imgchr.com/i/i26jts)
[![i2c9XT.png](https://s1.ax1x.com/2018/10/30/i2c9XT.png)](https://imgchr.com/i/i2c9XT)
#### Reference
Books: Deep learning



